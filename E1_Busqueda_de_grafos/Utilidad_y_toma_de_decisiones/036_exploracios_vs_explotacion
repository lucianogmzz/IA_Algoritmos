# Luciano Alejandro Gómez Muñoz 22310214 6E2.

# Breve explicación:
# Este programa simula el dilema de exploración vs explotación en un grafo. 
# El agente debe decidir si explora nuevas rutas o explota el mejor camino conocido para llegar a un objetivo.

# Librerías necesarias
import networkx as nx  # Para crear y manejar grafos
import matplotlib.pyplot as plt  # Para visualizar los grafos
import random  # Para realizar elecciones aleatorias

# Crear un grafo dirigido
G = nx.DiGraph()

# Agregar nodos (estados)
states = ['Start', 'A', 'B', 'C', 'Goal']
G.add_nodes_from(states)

# Agregar aristas (acciones posibles) con recompensas
G.add_edge('Start', 'A', reward=1)
G.add_edge('Start', 'B', reward=2)
G.add_edge('A', 'C', reward=2)
G.add_edge('B', 'C', reward=1)
G.add_edge('C', 'Goal', reward=5)

# Visualizar el grafo
pos = nx.spring_layout(G)
edge_labels = nx.get_edge_attributes(G, 'reward')
nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=10)
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
plt.title("Grafo para Exploración vs Explotación")
plt.show()

# Inicializar Q-table (valores de acciones)
Q = {state: {neighbor: 0 for neighbor in G.successors(state)} for state in G.nodes()}

# Parámetros
epsilon = 0.3  # 30% del tiempo exploramos
alpha = 0.5  # Tasa de aprendizaje
gamma = 0.9  # Factor de descuento
episodes = 500  # Número de episodios de entrenamiento

# Función para elegir acción
def choose_action(state):
    if random.uniform(0, 1) < epsilon:
        return random.choice(list(Q[state].keys()))  # Explora
    else:
        max_q = max(Q[state].values())
        best_actions = [action for action, value in Q[state].items() if value == max_q]
        return random.choice(best_actions)  # Explota

# Entrenamiento
for episode in range(episodes):
    state = 'Start'  # Siempre empieza desde "Start"
    while state != 'Goal':
        action = choose_action(state)
        reward = G[state][action]['reward']
        next_state = action
        old_q = Q[state][action]
        max_future_q = max(Q[next_state].values(), default=0)
        # Actualizar valor Q
        Q[state][action] = old_q + alpha * (reward + gamma * max_future_q - old_q)
        state = next_state  # Avanza al siguiente estado

# Mostrar la Q-table final
print("\nQ-Table aprendida:")
for state in Q:
    for action in Q[state]:
        print(f"Desde {state} hacia {action}: {Q[state][action]:.2f}")

# Mostrar política final
print("\nPolítica final basada en explotación:")
for state in Q:
    if Q[state]:  # Si tiene acciones disponibles
        best_action = max(Q[state], key=Q[state].get)
        print(f"Desde {state} ir hacia {best_action}")
